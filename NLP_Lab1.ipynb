{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqc8NSE41pQqBOMimM26EU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MRazin172/NLP_Lab1/blob/main/NLP_Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLiCbbtxim94",
        "outputId": "c4307ff2-3214-4fb4-b825-de6acdc90510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Word', 'tokenization', 'separates', 'text', 'into', 'individual', 'words', '.']\n",
            "Sentence Tokenization: ['Sentence tokenization splits text into separate sentences.', 'It handles various punctuation marks.']\n",
            "Punctuation-based Tokenizer: ['This', 'is', 'a', 'text', 'with', ',', 'punctuation', 'marks', '!', 'It', 'needs', 'separation', '.']\n",
            "Treebank Word Tokenizer: ['Treebank', 'tokenizer', 'splits', 'text', 'using', 'the', 'Penn', 'Treebank', 'conventions', '.']\n",
            "Tweet Tokenizer: ['Tweet', 'tokenizer', 'handles', '@usernames', 'and', '#hashtags', 'effectively', '!', '#NLTK']\n",
            "Multi-Word Expression Tokenizer: ['Multi-word', 'expression', 'tokenizer', 'treats', 'certain', 'phrases', 'as', 'single', 'tokens.']\n",
            "TextBlob Word Tokenize: ['TextBlob', 'provides', 'a', 'simple', 'interface', 'for', 'word', 'tokenization']\n",
            "spaCy Tokenizer: ['spaCy', 'tokenizer', 'breaks', 'text', 'into', 'tokens', 'using', 'advanced', 'linguistic', 'rules', '.']\n",
            "Gensim word tokenizer: ['Gensim', 'provides', 'tools', 'for', 'word', 'tokenization', 'suitable', 'for', 'processing', 'large', 'text', 'corpora']\n",
            "Tokenization with Keras: ['keras', 'offers', 'efficient', 'tokenization', 'methods', 'for', 'deep', 'learning', 'tasks']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, TreebankWordTokenizer, MWETokenizer\n",
        "from textblob import TextBlob\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Word tokenization separates text into individual words.\"\n",
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", words)\n",
        "\n",
        "text = \"Sentence tokenization splits text into separate sentences. It handles various punctuation marks.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sentences)\n",
        "\n",
        "text = \"This is a text with, punctuation marks! It needs separation.\"\n",
        "tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
        "print(\"Punctuation-based Tokenizer:\", tokens)\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Treebank tokenizer splits text using the Penn Treebank conventions.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Treebank Word Tokenizer:\", tokens)\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "text = \"Tweet tokenizer handles @usernames and #hashtags effectively! #NLTK\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tweet Tokenizer:\", tokens)\n",
        "\n",
        "tokenizer = MWETokenizer([('multi', 'word', 'expression')])\n",
        "text = \"Multi-word expression tokenizer treats certain phrases as single tokens.\"\n",
        "tokens = tokenizer.tokenize(text.split())\n",
        "print(\"Multi-Word Expression Tokenizer:\", tokens)\n",
        "\n",
        "text = \"TextBlob provides a simple interface for word tokenization.\"\n",
        "blob = TextBlob(text)\n",
        "tokens = blob.words\n",
        "print(\"TextBlob Word Tokenize:\", tokens)\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"spaCy tokenizer breaks text into tokens using advanced linguistic rules.\"\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"spaCy Tokenizer:\", tokens)\n",
        "\n",
        "text = \"Gensim provides tools for word tokenization, suitable for processing large text corpora.\"\n",
        "tokens = list(tokenize(text))\n",
        "print(\"Gensim word tokenizer:\", tokens)\n",
        "\n",
        "text = \"Keras offers efficient tokenization methods for deep learning tasks.\"\n",
        "tokens = text_to_word_sequence(text)\n",
        "print(\"Tokenization with Keras:\", tokens)\n"
      ]
    }
  ]
}